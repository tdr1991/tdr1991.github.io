{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Deep learning notes\n",
    "date: 2018-06-10 \n",
    "categories: [deep learning]\n",
    "tags: [deep learning]\n",
    "---\n",
    "Deep learning notes\n",
    "<!--more-->\n",
    "\n",
    "# part II - Deep Network: Modern Practices\n",
    "## Chapter6 - Deep Feedforward Networks\n",
    "### 6.2 Gradient-Based Learning\n",
    "For feedforward neural networks, it is important to initialize all weights to small random values. The biases may be initialized to zero or to small positive values.\n",
    "#### 6.2.1 Cost Function\n",
    "In most cases, our patametric model defines a distribution $p(y|x;\\theta)$ and we simply use the principle of maximum likelihood. This mean we use the cross-entropy between the training data and the model's predictions as the cost function.  \n",
    "Mean squared error and mean absolute error often lead to poor results when used with gradient-based optimization. Some output units that saturate produce very small gradients when combined with these cost functions.\n",
    "#### 6.2.2 Output Units\n",
    "In general, if we define a conditional distribution $p(y|x;\\theta)$, the principle of maximum likelihood suggests we use $-logp(y|x;\\theta)$.  \n",
    "It has been reported that gradient-based optimization of conditional Gaussian mixtures can be unreliable, in part because one gets divisions which can be numberically unstable. One solution is to clip gradients, while another is to scale the gradients heuristically.\n",
    "### 6.3 Hidden Units\n",
    "Unless indicated otherwise, most hidden units can be described as accepting a vector of inputs $x$, computing an affine transformation $z=W^Tx+b$, and then applying an element-wise nonlinear function $g(z)$. Most hidden units are distinguished from each other only by the choice of the form of the activation function $g(z)$.\n",
    "#### 6.3.1 Rectified linear units and their generalizations\n",
    "Rectified linear units are typically used on top of an affine transformation:\n",
    "$$h=g(W^T+b).$$\n",
    "When initializing the parameters of the affine transformation, it can be a good practice to set all element of $b$ to a small positive value, such as 0.1.  \n",
    "One drawback to rectified linear units is that they canot learn via gradient-based methods on examples for which their activation is zero. Various generalizations of rectified linear units guarantee that they receive gradient everywhere.  \n",
    "Three generalizations of rectified linear units are based on using a nonzero slope $\\alpha_i$ when $z_i<0:h_i=g(z,\\alpha)_i=max(0,z_i)+\\alpha_imin(0,z_i)$. Absolute value rectification fixes $\\alpha_i=-1$ to obtain $g(z)=|z|$. A leaky ReLU fixes $\\alpha_i$ to a small value like 0.01, while a parametric ReLU, or PReLU, treats $\\alpha_i$ as a learnable parameter.  \n",
    "Maxout units generalize rectified linear units further. Each maxout unit then outputs the maximum element of one of these groups:\n",
    "$$g(z)_i=\\operatorname*{max}_{j\\in G^{(i)}}z_j.$$\n",
    "Where $G^{(i)}$ is the set of indices into the inputs for group $i,\\{(i-1)k+1,...,ik\\}$.  \n",
    "Rectified linear units and all these generalizations of them are based on the principle that models are easier to optimize if their behavior is closer to linear. This same general priciple of using linear behavior to obtain easier optimization also applied in other contexts besides deep linear networks.\n",
    "### 6.4 Architecture Design\n",
    "#### 6.4.1 Universal approximation properties and depth\n",
    "Specifically, the universal approximation theorem states that a feedforward network with a linear output layer and at leas one hidden layer with any \"squashing\" activation function can approximate any Borel measurable function from one finite-dimensional space to another with any desired nonzero amount of error, provide that the network is given enough hidden units. \n",
    "#### 6.4.2 Other architectural considerations\n",
    "![Eï¬€ect of number of parameters.](/assets/2018-06-10/6-1.png)\n",
    "From the picture shows that increasing the number of parameters in layers of convolutional networks without increasing their depth is not nearly as effective at increasing test set performance. We observe that shallow models in this context overfit at around 20 million patameters while deep ones can benefit from having over 60 million. This suggests that using a deep model expresses a useful preference over the space of functions the model can learn.\n",
    "### 6.5 Historical notes\n",
    "Most of the improvement in neural network performance from 1986 to 2015 can be attribute to two factors. First, barger databases have reduced the degree to which statistical generalization is a challenge for neural networks. Second, neural networks have become much larger, because of more powerful computers and better software infrastructure. One of these algorithmic changes was the replacement of mean squared error with the cross-entropy family of loss functions.  \n",
    "A small number algorithmic changes have also improved the performance of neural networks noticeably. The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, which had previously sufferd from saturation and slow learning when using the mean squared error loss. The other major algorithmic changs that has greatly improved the performance of feedforward networks was the replacement of sigmoid hidden units with piecewise linear hidden units, such as rectified linear units. Rectified linear can be influenced by neuroscience.\n",
    "## Chapter 7 Regularization for deep learning\n",
    "###  7.1 Patameter norm penalties\n",
    "We donete the regularized objective function by $\\tilde{J}$\n",
    "$$\\tilde{J}(\\theta;X,y)=J(\\theta;X,y)+\\alpha\\Omega(\\theta)$$\n",
    "For neural networks, we typically choose to use a parameter norm penalty $\\Omega$ that penalized only the weights of the affine transformation at each layer and leaves the biases unregularized. Also, regularizing the bias parameters can introduce a significant amount of underfitting.  \n",
    "It is still reasonable to use the same weight decay at all layers just to reduce the size of search space.\n",
    "### 7.12 Dropout\n",
    "One of advantage of dropout is that it is very computationally cheap. Another significant advantage of droput is that it does not significantly limit the type of model or training procedure that can be used.  \n",
    "## Chapter 8 Optimization for Trainig Deep Models\n",
    "\n",
    "# Part I:Applied math and machine learning basics\n",
    "## Charpter 5 Machine learning basics\n",
    "### 5.1 Learing algorithms\n",
    "A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.  \n",
    "## 5.2 Capacity, overfitting and underfitting\n",
    "### 5.2.2 Regularization\n",
    "Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error.\n",
    "## 5.8 Unsupversied learning algorithms\n",
    "There are multiple ways of defining a simple representation. Three of the most common include lower-dimensional representations, sparse representaions, and independent representations. Low-dimensional representations attempt to compress as much information about $x$ as possible in a small representation. Sparse representations embed the dataset into a representation whose entries are mostly zeros for most inputs. The use of sparse representation typiclly requires increasing the dimensionality of the representation, so that the representaion becoming mostly zeros does not dicard too much information. This results in an overall structure of the representation that tends to distribute data along the axes of the representation space. Independent representations attempt to disentangle the sources of variation underlying the data distrubution such that the dimensions of the representation are statistically independent.\n",
    "## 5.10 Building a machine learning algorithm\n",
    "Nearly all deep learning algorithms can be described as particular instance of a fairly simple recipe: combine a specification of a dataset, a cost function, an optimization procedure and a model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "123px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
