{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_data = [338., 333., 328, 207., 226., 25., 179., 60., 208., 606.]\n",
    "y_data = [640., 633., 619., 393., 428., 27., 193., 66., 226., 1591.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(-200, -100, 1) #bias\n",
    "y = np.arange(-5, 5, 0.1) #weight\n",
    "Z = np.zeros((len(x), len(y)))\n",
    "X, Y = np.meshgrid(x, y)\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(y)):\n",
    "        b = x[i]\n",
    "        w = y[j]\n",
    "        Z[j][i] = 0\n",
    "        for n in range(len(x_data)):\n",
    "            Z[j][i] = Z[j][i] + (y_data[n] - b - w * x_data[n]) ** 2\n",
    "        Z[j][i] = Z[j][i] / len(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x252cfda7e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ydata = b + w * xdata\n",
    "b = -120 #initial b\n",
    "w = -4 #initial w\n",
    "lr = 1 # learning rate\n",
    "iteration = 100000\n",
    "# store initial values for plotting\n",
    "b_history = [b]\n",
    "w_history = [w]\n",
    "\n",
    "# Adagrad 算法调节学习率\n",
    "lr_b = 0\n",
    "lr_w = 0\n",
    "\n",
    "# iterations\n",
    "for i in range(iteration):\n",
    "    b_grad = 0.0\n",
    "    w_grad = 0.0\n",
    "    for n in range(len(x_data)):\n",
    "        b_grad = b_grad - 2.0 * (y_data[n] - b - w * x_data[n]) * 1.0\n",
    "        w_grad = w_grad - 2.0 * (y_data[n] - b - w * x_data[n]) * x_data[n]\n",
    "    lr_b = lr_b + b_grad ** 2\n",
    "    lr_w = lr_w + w_grad ** 2\n",
    "    \n",
    "    \n",
    "    b = b - lr / np.sqrt(lr_b) * b_grad\n",
    "    w = w - lr / np.sqrt(lr_w) * w_grad\n",
    "    # store parameters for plotting\n",
    "    b_history.append(b)\n",
    "    w_history.append(w)\n",
    "# plot the figure\n",
    "plt.contourf(x, y, Z, 50, alpha=0.5, cmap=plt.get_cmap(\"jet\"))\n",
    "plt.plot([-188.4], [2.67], \"x\", ms=12, markeredgewidth=3, color=\"orange\")\n",
    "plt.plot(b_history, w_history, \"o-\", ms=3, lw=1.5, color=\"black\")\n",
    "plt.xlim(-200, -100)\n",
    "plt.ylim(-5, 5)\n",
    "plt.xlabel(r'$b$', fontsize=16)\n",
    "plt.ylabel(r'$w$', fontsize=16)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "定义损失函数$L(\\theta)$，$\\theta$表示参数集合，$\\theta=\\{\\theta_1,\\theta_2,...,\\theta_n\\}$。\n",
    "初始化$\\theta为\\theta^0，\\theta^0=[\\theta_1^0, \\theta_2^0,...,\\theta_n^0]$。\n",
    "$\\nabla L(\\theta)=[\\partial L(\\theta_1)/\\partial \\theta_1, \\partial L(\\theta_2)/\\partial \\theta_2,..., \\partial L(\\theta_n)/\\partial \\theta_n]$\n",
    "$$\\theta^1 = \\theta^0-\\eta\\nabla L(\\theta^0)$$\n",
    "$$\\theta^k = \\theta^{k-1}-\\eta\\nabla L(\\theta^{k-1})$$\n",
    "\n",
    "# 自适应学习率\n",
    "如果损失函数对于某个给定模型参数的偏导保持相同的符号，那么学习率应该增加。反之，减少。\n",
    "## AdaGrad\n",
    "该适应性学习率使用了所有历史梯度。较大梯度其学习率下降更快，反之，较慢。缺点是某些案例中学习率会变得太小，学习率的单调下降也会使网络停止学习。  \n",
    "$$ g \\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)}) $$\n",
    "$$ r \\leftarrow r + g \\bigodot g $$\n",
    "$$ \\Delta \\theta \\leftarrow -\\frac{\\epsilon}{\\delta + \\sqrt{r}} \\bigodot g $$\n",
    "$$ \\theta \\leftarrow \\theta + \\Delta \\theta $$\n",
    "其中的 $ \\delta $ 是防止分母出现0,$ \\bigodot $表示对应元素之间的乘积。\n",
    "## AdaDelta\n",
    "AdaDelta使用短期历史梯度信息而缩放学习率。其可克服AdaGrad的学习率过快减少至零，其将累计历史梯度信息的范围限制在固定窗口w内。\n",
    "$$ E[g^2]_t = \\rho E[g^2]_{t-1} + (1-\\rho)g_t^2 $$\n",
    "$$ \\Delta x_t = -\\frac{RMS[\\Delta x]_{t-1}}{RMS[g]_t} g_t $$\n",
    "$$ E[\\Delta x^2]_t = \\rho E[\\Delta x^2]_{t-1} + (1-\\rho)\\Delta x_t^2 $$\n",
    "$$ x_{t+1} = x_t + \\Delta x_t $$\n",
    "\n",
    "## RMSProp\n",
    "RMSProp算法修改AdaGrad以在非凸情况下表现更好，它改变梯度累加为指数加权的移动平均值，从而丢弃距离较远的历史梯度信息。\n",
    "其标准式：\n",
    "$$ g \\leftarrow \\frac{1}{m}\\nabla_\\theta \\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)}) $$\n",
    "$$ r \\leftarrow \\rho r + (1-\\rho) g \\bigodot g $$\n",
    "$$ \\Delta \\theta \\leftarrow -\\frac{\\epsilon}{\\sqrt{\\delta + r}} \\bigodot g $$\n",
    "$$ \\theta \\leftarrow \\theta + \\Delta \\theta $$\n",
    "结合 Nesterov 动量而提升经典 RMSProp 算法：\n",
    "$$ \\tilde \\theta \\leftarrow \\theta + \\alpha v $$\n",
    "$$ g \\leftarrow \\frac{1}{m}\\nabla_{\\tilde \\theta} \\sum_{i=1}^m L(f(x^{(i)};\\theta),y^{(i)}) $$\n",
    "$$ r \\leftarrow \\rho r + (1-\\rho) g \\bigodot g $$\n",
    "$$ v \\leftarrow \\alpha v - \\frac{\\epsilon}{\\sqrt{r}} \\bigodot g $$\n",
    "$$ \\theta \\leftarrow \\theta + v $$\n",
    "## Adam\n",
    "Adam 算法的提出者描述其为两种随机梯度下降扩展式的优点集合，即：\n",
    "* 适应性梯度算法（AdaGrad）为每一个参数保留一个学习率以提升在稀疏梯度（即自然语言和计算机视觉问题）上的性能。\n",
    "* 均方根传播（RMSProp）基于权重梯度最近量级的均值为每一个参数适应性地保留学习率。这意味着算法在非稳态和在线问题上有很有优秀的性能。  \n",
    "Adam 算法同时获得了 AdaGrad 和 RMSProp 算法的优点。Adam 不仅如 RMSProp 算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值（即有偏方差/uncentered variance）。具体来说，算法计算了梯度的指数移动均值（exponential moving average），超参数 $ \\rho_1 $  和 $ \\rho_2 $ 控制了这些移动均值的衰减率。\n",
    "$$ t \\leftarrow t + 1 $$ \n",
    "$$ g_t \\leftarrow \\nabla_\\theta f_t(\\theta_{t-1}) $$\n",
    "$$ s_t \\leftarrow \\rho_1 s_{t-1} + (1-\\rho_1) g_t $$\n",
    "$$ r_t \\leftarrow \\rho_2 r_{t-1} + (1-\\rho_2) g_t \\bigodot g_t $$\n",
    "$$ \\hat s_t \\leftarrow \\frac{s_t}{1-\\rho_1^t} $$\n",
    "$$ \\hat r_t \\leftarrow \\frac{r_t}{1-\\rho_2^t} $$\n",
    "$$ \\Delta \\theta = -\\epsilon \\frac{\\hat s_t}{\\delta + \\sqrt{\\hat r_t}}  $$\n",
    "$$ \\theta_t \\leftarrow \\theta_{t-1} + \\Delta \\theta $$\n",
    "\n",
    "# 随机梯度下降\n",
    "选取一个样本做损失函数\n",
    "\n",
    "# Feature Scaling\n",
    "$n$个样本，$m$个特征,数据 x  \n",
    "第$j$个特征的均值，$m_j=\\frac{\\sum_{i=1}^n  x_{ij}}{n}$  \n",
    "第$j$个特征的标准差，$\\sigma_j=\\sqrt{\\frac{\\sum_{i=1}^n (x_{ij}-m_j)^2}{n-1}}$  \n",
    "$$x_{ij}^{*}=\\frac{x_{ij}-m_j}{\\sigma_j}$$\n",
    "那么经过特征缩放后每个特征的均值为0，方差为1\n",
    "\n",
    "参考文章：  \n",
    "<https://www.jiqizhixin.com/articles/adam-iclr-2018>  \n",
    "<https://www.jiqizhixin.com/articles/2018-07-03-14>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
